name: 'claraval_dbt'
version: '1.0.0'
config-version: 2

vars:
  DBT_TIME_ZONE: "America/Sao_Paulo"

  # upstream_prod package variables
  upstream_prod_database: claraval-datawarehouse
  upstream_prod_env_schemas: True
  upstream_prod_enabled: "{{ env_var('DBT_UPSTREAM_PROD_ENABLED', 'False') | as_bool }}"
  upstream_prod_fallback: True
  upstream_prod_prefer_recent: "{{ env_var('DBT_UPSTREAM_PROD_PREFER_RECENT', 'False') | as_bool }}"
  disable_dbt_artifacts_autoupload: true

on-run-start:
  - '{% do adapter.create_schema(api.Relation.create(target.project, "audit")) %}'

# This setting configures which "profile" dbt uses for this project.
profile: "claraval_dbt"

# The `model-paths` config
model-paths: ["models"]
analysis-paths: ["analysis"]
test-paths: ["tests/specific", "tests/unit_tests"]
seed-paths: ["data"]
macro-paths: ["macros", "tests/generic", "model_templates"]
snapshot-paths: ["snapshots"]

target-path: "target" # directory which will store compiled SQL files
clean-targets: # directories to be removed by `dbt clean`
  - "target"
  - "dbt_modules"

# Configuring models
# Keep in mind that the way these configs work is strongly affected by
# the macro in `macros/get_custom_schema.sql`.

models:
  bp_dbt:
    +required_docs: true
    +persist_docs:
      relation: true
      columns: true
    materialized: view
    # for dbt Python models
    jar_file_uri: "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.13-0.31.0.jar"

    ml_features:
      +schema: datamart
    staging:
      +schema: staging

    analytics:
      +schema: analytics

    functions:
      tag_score:
        +post-hook: "{{ custom_grant_access_to(this, [('bp-datawarehouse', 'staging'), ('bp-datawarehouse', 'masterdata'), ('bp-datawarehouse', 'datamart'), ('bp-datawarehouse', 'ml_models')]) }}"